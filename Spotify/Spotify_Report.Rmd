---
title: "Spotify Project"
author: "Jon Breen"
date: "6/21/2021"
output: html_document
---

## Synopsis

The purpose of this report is to explore a Spotify data set in order to create a model capable of classifying music based on a specific set of characteristics. This will be done using a variety of models and comparing the results in order to select the best one. Once a model has been created and tested the results will be analyzed and used for further exploration. 

## Initial Preparation and Obtaining the Data

Before we can start looking into the data set there are a few things that we must do in preparation. The first would be to prepare the necessary packages that will be required to continue with this process. The second would be to actually obtain the data set.

### Loading Required Libraries

```{r echo = TRUE}
suppressMessages(library(tidyverse))
suppressMessages(library(caret))
suppressMessages(library(corrplot))
suppressMessages(library(gbm))
suppressMessages(library(DescTools))
```

### Reading in the Data

```{r echo = TRUE}
suppressMessages(spotify <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv'))
```

## Data Exploration

Now that we have our data the next step is to examine it. The data set contains 23 variables, but many of them aren't able to contribute anything of value for the purposes of this report. There are also other variables that need to be addressed.

### Variable Selection and Adjusting

```{r echo = TRUE}
head(spotify)
```

As you can see there are a variety of variables in this data set, but previously mentioned only a select few of them are beneficial to this report. The variables that are of importance are the playlist_genre, which will be our dependent variable, and the measurable track characteristics (such as loudness, danceability, etc.)

```{r echo = TRUE}
spotify1 <- spotify %>% select(playlist_genre:duration_ms) %>% select(-playlist_subgenre)

head(spotify1)
```

After removing the unnecessary variables we can once more take a quick look at the data. What we're left with is quantifiable values that can, hopefully, be used to classify a song into a specific genre. However, there are a few variables that need to be looked at. Specifically playlist_genre, key, and mode. Based on the range of values it would probably be best to convert them into factor variables.

```{r echo = TRUE}
spotify1 <- spotify1 %>% mutate(across(c(playlist_genre, key, mode), factor))
```

### Brief Look at the Remaining Data
With only the appropriate variables remaining and converted to the proper data type, we can take a brief look at the remaining variables to make sure we can proceed. Playlist_genre, key, and mode will be left out due to them being factor variables.

```{r echo = TRUE}
corrplot(cor(spotify1 %>% select(-playlist_genre, -key, -mode)), type = "upper", diag = FALSE)
```

Looking at the results of the corrplot there doesn't appear to be a lot of collinearity among the data. The only variable that might be of concern is energy due to its correlation with loudness and acousticness. It will remain for the time being, but it's something to keep in mind.

## Model Creation
Once the data has been fully explored and cleaned we an move on to the meat of this report - the model creation. A few steps will be taken in order to create sufficient models. First, we will create a training set and a test set. The training set will be used to build the models and select the best one, while the test will server to, well, test the model.

### Creating Train/Test Split

```{r echo = TRUE}
set.seed(123)
inTrain1 <- createDataPartition(y = spotify1$playlist_genre, p = .7, list = FALSE)
training1 <- spotify1[inTrain1,]
testing1 <- spotify1[-inTrain1,]
```

### Initial Models

```{r echo = TRUE, cache = TRUE}
control <- trainControl(method = "cv", number = 5)

rfGrid <- expand.grid(mtry = c(1:5))
gbmGrid <- expand.grid(interaction.depth = c(3, 5), 
                       n.trees = (1:4)*100, 
                       shrinkage = 0.1, 
                       n.minobsinnode = c(5, 10))

mod_rf <- train(playlist_genre ~ ., data = training1, method = "rf", trControl = control,
                preProcess = c("center", "scale"), tuneGrid = rfGrid)

mod_gbm <- train(playlist_genre ~ ., data = training1, method = "gbm", trControl = control,
                 verbose = FALSE, preProcess = c("center", "scale"), tuneGrid = gbmGrid)

mod_knn <- train(playlist_genre ~ ., data = training1, method = "knn", trControl = control,
                 preProcess = c("center", "scale"))
```

Three different models were used initially: Random Forest, Gradient Boosting, and K Nearest Neighbors. A grid search was used for the RF (although caret only allows for one hyperparameter to be tuned in this case) and the GBM models while the KNN model was ran with the defaults. Caret also ran a 5-fold cross validation on the models so they can be compared.

### Basic Model Tuning and Selection

We can start the turning and selection process by looking at some of the results we obtained.

```{r echo = TRUE}
mod_rf$results %>% arrange(desc(Accuracy)) %>% head()
mod_gbm$results %>% arrange(desc(Accuracy)) %>% head()
mod_knn$results %>% arrange(desc(Accuracy)) %>% head()
```

Looking at the results from the cross validation it seems that the Random Forest model provides the best accuracy. Since Caret only allows for minimal tuning with this particular there isn't much more improvement to do. Another possible step would be look at the GBM model and alter the grid search, however there doesn't seem to be any clear indication of a direction to move. The top models use a variety of n.trees, n.minobsinnode, and interaction.depth parameters so nothing stands out immediately. It's unlikely that there will be any significant increases in the accuracy if more time is spent tuning, so it seems like the Random Forest model will be the best choice. 

## Testing the Model

The last step in this model building process is to test our selected model against the test set. We will then look at the results and try to garner some conclusions.

### Results

```{r echo = TRUE}
pred_rf <- predict(mod_rf, testing1)

results_rf <- confusionMatrix(pred_rf, testing1$playlist_genre) %>% print()
```

### Analyzing the Results

After looking at the results of our model, there are a few things that might be of importance to note. First, and most obvious, is the accuracy.

```{r echo = TRUE}
round(results_rf$overall, 4)
```

The model ended up predicting the genre of songs in the test set with an accuracy of 55.77%. While this might seem low at first, it's worth considering the fact that this accuracy is an improvement of nearly 40% on average when randomly guessing. Despite that, though, the resulting accuracy is still less than ideal.

The accuracy isn't the only thing that should be looked at though. The prediction results also tell another story that can be dug into.

```{r echo = TRUE}
results_rf$byClass[,8]
```

These numbers are the balanced accuracy ratings by genre. While the overall accuracy wasn't particularly high, some of the by genre ratings are interesting. For example, rock music has a balanced accuracy of 84.97%. Conversely, pop music has a balanced accuracy rating of only 59.92%. That is a significant difference. This could lead to the question - is there a relationship between song genre and predictability. Extending from this, one might ask the question - Is there a relationship between genres and their likelihood to be confused with one another. 

## Further Examining the Relationship Between Genres
So a model has been created to try and predict the genre of a song based on some defining measurements and the results of that model have been analyzed. That analysis has pointed out another interesting path to take to do further testing. There are a few different things that can be done.

### Building a Model Without "pop" Genre
As the output of our test predictions showed, there are clearly some imbalances between the specific genres in terms of predictability. Pop music had the lowest while rock music had the highest. In an attempt to see just how much of a difference it would, another model will be created to try and predict against a test set. This model will be trained and tested on sets without the pop genre and the results will, once again, be looked at and compared to the previous results.

```{r echo = TRUE}
spotify3 <- spotify1 %>% filter(playlist_genre != "pop")
spotify3 <- droplevels(spotify3)

set.seed(789)
inTrain3 <- createDataPartition(y = spotify3$playlist_genre, p = .7, list = FALSE)
training3 <- spotify3[inTrain3,]
testing3 <- spotify3[-inTrain3,]
```

```{r echo = TRUE, cache = TRUE}
mod_rf3 <- train(playlist_genre ~ ., data = training3, method = "rf", trControl = control,
                 preProcess = c("center", "scale"), tuneGrid = rfGrid)
```

This time around just one model was created. It should be satisfactory as the purpose at this point isn't to create the most accurate model, but rather to simply compare some basic results.

```{r echo = TRUE}
pred_rf3 <- predict(mod_rf3, testing3)

results3 <- confusionMatrix(pred_rf3, testing3$playlist_genre) %>% print()
```

As expected there is an increase in the accuracy of this new model. The increase is about 9%, reaching 64.27%. The balanced accuracy ratings are very similar to the previous results with only minor changes. After looking at all the information it's hard to draw concrete conclusion. While there was a noticeable increase in the model's accuracy, that could be a result of having fewer categories to classify a song into. More tests will have to be done to help support any conclusions.

### Examining Tracks Tied to Multiple Genres
Since the prediction model wasn't able to create any decisive conclusions, one thing to do is look at tracks that are listed under multiple genres. This might help us to see genres that are often mixed up with one another.

```{r echo = TRUE}
spotify_track <- spotify %>% select(track_id, playlist_genre:duration_ms) %>% select(-playlist_subgenre)

spotify_track %>% group_by(playlist_genre) %>% summarise(proportion = n()/nrow(spotify_track))

multi <- spotify_track %>%
            group_by(track_id, playlist_genre) %>%
            count() %>%
            group_by(track_id) %>%
            summarise(count = n()) %>%
            filter(count >= 2)

spotify_multi <- right_join(spotify_track, multi, by = "track_id")

spotify_multi %>% 
      group_by(playlist_genre) %>%
      summarise(proportion = n()/nrow(spotify_multi))
```

There are a few pieces of information worth looking at here. Firstly, we can look at the baseline proportions for each genre in the dataset. This simply lets us know what our starting point is. Next, we have a new set of proportions. These are the proportions from our new dataset that only contains tracks listed under two or more genres. This tells us what genres are most commonly assigned to the same tracks. 

The results seem to line up with the surface level observations from the prediction models - rock music is the least likely to be tied with other genres while pop is the most. This can be seen by the dramatic decrease and increase in the proportions. Rock music initially made up 15.1% of the dataset, but when looking at tracks that are under two more genres, rock music only makes up 5.20%. Conversely, pop music made up 16.8% of the dataset, but after the filtering makes up 28.3%. These results seem a bit more telling than than the balance accuracy ratings from our predictive models, but there's still more we can do.

### Hypothesis Test

Another way to see if there's any form of relationship between genres and their likelihood to be misclassified is to do a hypothesis test. Our null assumption will be "There is no relationship between genre and misclassification" and our alternative hypothesis will be "There is a relationship between genre and misclassification".

```{r echo = TRUE}
multi2 <- spotify_track %>%
              group_by(track_id, playlist_genre) %>%
              count() %>%
              group_by(track_id) %>%
              summarise(count = n())

spotify_hyp <- right_join(spotify_track, multi2, by = "track_id")

chisq <- chisq.test(spotify_hyp$playlist_genre, spotify_hyp$count) %>% print()
```

According to the Chi-Squared hypothesis test there does appear to be a statistically significant relationship between the number of genres a track is listed as and the particular genres associated with a track. We can run another quick test to see how strong the relationship is.

```{r echo = TRUE}
CramerV(table(spotify_hyp$playlist_genre, spotify_hyp$count))
```

According to this the relationship is not high in general at only a .086 out of 1. Despite that, however, there still might be something to gain from this test. After all, there are multiple genres and some might contribute less than others and this could weaken the results. It could still be worthwhile to look into the results further for the sake of being thorough.

```{r echo = TRUE}
corrplot(chisq$residuals, is.cor = FALSE, title = "Residual Values of # of Genres Versus Types of Genres", mar = c(1, 1, 1, 1))
```

These are the residual values from our hypothesis test. These values seem to match our initial observations from looking at multi-genre track proportions. Pop and rock music have the largest residuals (with pop being positive and rock negative), although latin music has some moderately high residuals at the higher count values which might come as a surprise. This supports the thought that some genres contribute much more to the relationship being examined. We can visual the contributions to get a better understanding.

```{r echo = TRUE}
contrib <- as.data.frame(100*chisq$residuals^2/chisq$statistic) %>% 
                rename(playlist_genre = spotify_hyp.playlist_genre, count = spotify_hyp.count)

contrib %>%
      group_by(playlist_genre) %>%
      ggplot(aes(playlist_genre, Freq, fill = count)) +
            geom_bar(stat = "identity") +
            ggtitle("Total Contribution % by Genre") +
            ylab("Contribution %")
```

By visualizing the contribution data it becomes very clear that rock and pop are the primary contributors. All the other data looked at prior to this point supports this discovery to some degree. Further research could be warranted to fully explore these genres in specific.

## Conclusion

After running a multitude of different tests and models, there are a few things that can be concluded. First, music is complicated. While a few variables had high degrees of correlation, the majority of them displayed no to very little correlation. After creating a few different models and looking at them closely the best accuracy that could be produced was about 55%. The second conclusion is that some genres might be more difficult to classify than others. This was seen in a variety of ways from the balanced accuracy numbers in the prediction results, the proportion of songs listed under two or more genres, and from the Chi-Squared hypothesis test. 